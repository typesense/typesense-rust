/*
 * Typesense API
 *
 * An open source search engine for building delightful search experiences.
 *
 * The version of the OpenAPI document: 30.0
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use ::std::{borrow::Cow, marker::PhantomData};
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct ConversationModelSchema<'a> {
    /// An explicit id for the model, otherwise the API will return a response with an auto-generated conversation model id.
    #[serde(rename = "id")]
    pub id: Cow<'a, str>,
    /// Name of the LLM model offered by OpenAI, Cloudflare or vLLM
    #[serde(rename = "model_name")]
    pub model_name: Cow<'a, str>,
    /// The LLM service's API Key
    #[serde(rename = "api_key", skip_serializing_if = "Option::is_none")]
    pub api_key: Option<Cow<'a, str>>,
    /// Typesense collection that stores the historical conversations
    #[serde(rename = "history_collection")]
    pub history_collection: Cow<'a, str>,
    /// LLM service's account ID (only applicable for Cloudflare)
    #[serde(rename = "account_id", skip_serializing_if = "Option::is_none")]
    pub account_id: Option<Cow<'a, str>>,
    /// The system prompt that contains special instructions to the LLM
    #[serde(rename = "system_prompt", skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<Cow<'a, str>>,
    /// Time interval in seconds after which the messages would be deleted. Default: 86400 (24 hours)
    #[serde(rename = "ttl", skip_serializing_if = "Option::is_none")]
    pub ttl: Option<i32>,
    /// The maximum number of bytes to send to the LLM in every API call. Consult the LLM's documentation on the number of bytes supported in the context window.
    #[serde(rename = "max_bytes")]
    pub max_bytes: i32,
    /// URL of vLLM service
    #[serde(rename = "vllm_url", skip_serializing_if = "Option::is_none")]
    pub vllm_url: Option<Cow<'a, str>>,
    #[serde(skip)]
    pub _phantom: PhantomData<&'a ()>,
}

impl<'a> ConversationModelSchema<'a> {
    pub fn new(
        id: Cow<'a, str>,
        model_name: Cow<'a, str>,
        history_collection: Cow<'a, str>,
        max_bytes: i32,
    ) -> Self {
        Self {
            id,
            model_name,
            api_key: None,
            history_collection,
            account_id: None,
            system_prompt: None,
            ttl: None,
            max_bytes,
            vllm_url: None,
            _phantom: PhantomData,
        }
    }
}
