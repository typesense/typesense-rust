/*
 * Typesense API
 *
 * An open source search engine for building delightful search experiences.
 *
 * The version of the OpenAPI document: 28.0
 * 
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct NlSearchModelCreateSchema {
    /// Name of the NL model to use
    #[serde(rename = "model_name", skip_serializing_if = "Option::is_none")]
    pub model_name: Option<String>,
    /// API key for the NL model service
    #[serde(rename = "api_key", skip_serializing_if = "Option::is_none")]
    pub api_key: Option<String>,
    /// Custom API URL for the NL model service
    #[serde(rename = "api_url", skip_serializing_if = "Option::is_none")]
    pub api_url: Option<String>,
    /// Maximum number of bytes to process
    #[serde(rename = "max_bytes", skip_serializing_if = "Option::is_none")]
    pub max_bytes: Option<i32>,
    /// Temperature parameter for the NL model
    #[serde(rename = "temperature", skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f64>,
    /// System prompt for the NL model
    #[serde(rename = "system_prompt", skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<String>,
    /// Top-p parameter for the NL model (Google-specific)
    #[serde(rename = "top_p", skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f64>,
    /// Top-k parameter for the NL model (Google-specific)
    #[serde(rename = "top_k", skip_serializing_if = "Option::is_none")]
    pub top_k: Option<i32>,
    /// Stop sequences for the NL model (Google-specific)
    #[serde(rename = "stop_sequences", skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,
    /// API version for the NL model service
    #[serde(rename = "api_version", skip_serializing_if = "Option::is_none")]
    pub api_version: Option<String>,
    /// Project ID for GCP Vertex AI
    #[serde(rename = "project_id", skip_serializing_if = "Option::is_none")]
    pub project_id: Option<String>,
    /// Access token for GCP Vertex AI
    #[serde(rename = "access_token", skip_serializing_if = "Option::is_none")]
    pub access_token: Option<String>,
    /// Refresh token for GCP Vertex AI
    #[serde(rename = "refresh_token", skip_serializing_if = "Option::is_none")]
    pub refresh_token: Option<String>,
    /// Client ID for GCP Vertex AI
    #[serde(rename = "client_id", skip_serializing_if = "Option::is_none")]
    pub client_id: Option<String>,
    /// Client secret for GCP Vertex AI
    #[serde(rename = "client_secret", skip_serializing_if = "Option::is_none")]
    pub client_secret: Option<String>,
    /// Region for GCP Vertex AI
    #[serde(rename = "region", skip_serializing_if = "Option::is_none")]
    pub region: Option<String>,
    /// Maximum output tokens for GCP Vertex AI
    #[serde(rename = "max_output_tokens", skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<i32>,
    /// Account ID for Cloudflare-specific models
    #[serde(rename = "account_id", skip_serializing_if = "Option::is_none")]
    pub account_id: Option<String>,
    /// Optional ID for the NL search model
    #[serde(rename = "id", skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
}

impl NlSearchModelCreateSchema {
    pub fn new() -> NlSearchModelCreateSchema {
        NlSearchModelCreateSchema {
            model_name: None,
            api_key: None,
            api_url: None,
            max_bytes: None,
            temperature: None,
            system_prompt: None,
            top_p: None,
            top_k: None,
            stop_sequences: None,
            api_version: None,
            project_id: None,
            access_token: None,
            refresh_token: None,
            client_id: None,
            client_secret: None,
            region: None,
            max_output_tokens: None,
            account_id: None,
            id: None,
        }
    }
}

